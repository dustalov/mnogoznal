{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import platform\n",
    "import csv\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import defaultdict\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для форматирования вывода\n",
    "BOLD = '\\033[1m'\n",
    "END = '\\033[0m'\n",
    "UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Извлечение слова из строки вида 'word#X:Y'\n",
    "def lexeme(str):\n",
    "    \"\"\"Возвращает word, если вид word#X\n",
    "    Возвращает word, freq, если вид word#X:freq\n",
    "    \"\"\"\n",
    "    if '#' in str:\n",
    "        word, tail = str.split('#', 1)\n",
    "    else:\n",
    "        word, tail = str, None\n",
    "\n",
    "    if tail:\n",
    "        if ':' in tail:\n",
    "            labels, tail = tail.split(':', 1)\n",
    "        else:\n",
    "            labels, tail = tail, None\n",
    "\n",
    "    if tail:\n",
    "        freq = float(tail)\n",
    "    else:\n",
    "        freq = 1\n",
    "    \n",
    "    return word, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synsets = {}  # Словарь {id -> список синсетов}\n",
    "index = defaultdict(list)  # Словарь {слово -> номера синсетов с упоминаниями}\n",
    "lexicon = set()  # Набор всех слов в базе\n",
    "\n",
    "# Считываем файл\n",
    "with open('watset-mcl-mcl-joint-exp-linked.tsv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)  \n",
    "\n",
    "    # Перебираем строки и заполняем переменные synsets и relations словами.\n",
    "    # Ключи - номер строки (с единицы).\n",
    "    # Значения - словари вида {слово -> частота}.\n",
    "    for row in reader:\n",
    "        synsets_dict = dict()\n",
    "        for word in row[2].split(', '):\n",
    "            if word:\n",
    "                key, value = lexeme(word)\n",
    "                synsets_dict[key] = value\n",
    "                \n",
    "        for word in row[4].split(', '):\n",
    "            if word:\n",
    "                key, value = lexeme(word)\n",
    "                synsets_dict[key] = value\n",
    "        synsets[int(row[0])] = synsets_dict\n",
    "        \n",
    "        # Закидываем номер строки в index для каждого слова.\n",
    "        for word in synsets[int(row[0])]:\n",
    "            index[word].append(int(row[0]))\n",
    "\n",
    "        # Обновляем лексикон базы данных\n",
    "        lexicon.update(synsets[int(row[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('dict', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = Pipeline([('dict', DictVectorizer()), ('tfidf', TfidfTransformer())])\n",
    "v.fit(synsets.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mystem_func(text):\n",
    "    \"\"\"Возвращает список, состоящий из списков вида [token, lemma, pos]\"\"\"\n",
    "    \n",
    "    coding = 'UTF-8'\n",
    "    if (platform.system() == 'Windows'):\n",
    "        coding = 'cp866'\n",
    "    \n",
    "    # Выполнение команды mystem\n",
    "    #command = \"echo '%s' | ./mystem -e %s -nidsc\" % (text, coding)\n",
    "    command = \"echo '%s' | mystem -e %s -nidsc\" % (text, coding)\n",
    "    proc = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, )\n",
    "    output = proc.communicate()[0] \n",
    "    \n",
    "    # Обработка результата в считываемый вид (массив строк)\n",
    "    string_output = str(output.decode(coding))\n",
    "    \n",
    "    #print(repr(string_output), '\\n')\n",
    "    \n",
    "    #string_output = string_output[3:]  # В начале вывода стоит кавычка и перевод строки - не значащая информация\n",
    "    string_output = string_output[:-2]  # В конце вывода стоит кавычка и перевод строки - не значащая информация\n",
    "    \n",
    "    string_output = string_output.replace(\"_\", \"\")  # Убираем незначащий символ \"_\" в выводе \n",
    "    sentences = string_output.split('\\s')  # Деление вывода на предложения\n",
    "    \n",
    "    #print(sentences, '\\n')\n",
    "\n",
    "    # Обработка каждого предложения по очереди\n",
    "    # Результат каждого предложения - элемент списка sentences_array\n",
    "    sentences_array = list()\n",
    "    for sentence in sentences:\n",
    "        #print(sentence, '\\n')\n",
    "        if (platform.system() == 'Windows'):\n",
    "            words = sentence.split(os.linesep)  # Деление предложений на слова\n",
    "        else:\n",
    "            words = sentence.split('/n')  # Деление предложений на слова\n",
    "            print(words)\n",
    "        words = [x for x in words if x != '']  # В некоторых местах появляются пустые элементы списка - удаляем\n",
    "        \n",
    "    \n",
    "        # Обработка каждой строки так, чтобы получить для слова его токен, лемму и часть речи.\n",
    "        # Результат для каждого слова записывается в список из трех элементов.\n",
    "        # Все списки хранятся в списке words_array\n",
    "        words_array = list()\n",
    "        for word_line in words:\n",
    "            if (len(word_line) == 1):  #  Случай, если попался знак пунктуации\n",
    "                token = word_line\n",
    "                buf = [word_line, 'PUNC']\n",
    "            \n",
    "            else:  #  Случай, если попалось слово\n",
    "                start_index = word_line.find('{',)\n",
    "                end_index = len(word_line) - 1\n",
    "                token = word_line[:start_index]  \n",
    "                \n",
    "                # Отбрасываем лишнюю информацию\n",
    "                buf = word_line[start_index+1:end_index].split(',')\n",
    "                buf = buf[0].split('=')\n",
    "                if (len(buf) > 2):\n",
    "                    del buf[2]\n",
    "              \n",
    "            # Оформляем результат в виде списка\n",
    "            buf.insert(0, token)\n",
    "            words_array.append(buf)\n",
    "        \n",
    "        sentences_array.append(words_array)\n",
    "    return sentences_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Трансформируем слова в начальную форму для дальнейшего поиска по базе данных\n",
    "def initial_form(sentences):\n",
    "    \"\"\"Возвращает список предложений со списками слов в начальной форме\"\"\"\n",
    "    initial_text_list = list()\n",
    "    for sentence in sentences:\n",
    "        initial_sentence_list = list()\n",
    "        for word_array in sentence:\n",
    "            initial_sentence_list.append(word_array[1])\n",
    "        initial_text_list.append(initial_sentence_list)\n",
    "    return initial_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punc = ['!','?', ',', '.', ';', '\"', \"'\"]\n",
    "\n",
    "# Рассчет векторного расстояния между словарем sentence_dict и синсетов, которые содержат word.\n",
    "# relation - переменная bool, означает проверку либо близких слов (значение в синсете = 1), \n",
    "# либо вспомогательных слов (значение в синсете < 1)\n",
    "def cos_func(sentence_dict, word, relation):\n",
    "    \"\"\"Возвращает словарь векторных расстояний синсетов и предложений\"\"\"\n",
    "    cos_result = dict()\n",
    "    for i in index[word]:\n",
    "        synset = synsets[i]\n",
    "        if (synset[word] == 1):\n",
    "            sim = cosine_similarity(v.transform(synset), v.transform(sentence_dict)).item(0)\n",
    "            sim = float(\"{0:.4f}\".format(sim))\n",
    "            if (sim != 0):\n",
    "                cos_result[i] = sim\n",
    "        elif (relation is True) :\n",
    "            sim = cosine_similarity(v.transform(synset), v.transform(sentence_dict)).item(0)\n",
    "            sim = float(\"{0:.4f}\".format(sim))\n",
    "            if (sim != 0):\n",
    "                cos_result[i] = sim\n",
    "    return cos_result\n",
    "\n",
    "# Поиск наилучшего синсета для ask_word из предложения words_list\n",
    "def cos_similar(words_list, ask_word):\n",
    "    \"\"\"Возвращает номер синсета для слова в предложении\"\"\"\n",
    "    #print(BOLD + ask_word + END, end=': ')  # Для проверки результатов\n",
    "    \n",
    "    words_list = [x for x in words_list if x not in punc] # Удаляем знаки пунктуации\n",
    "    words_list.remove(ask_word)  # Удаляем запрашиваемое слово для поиска только по контексту\n",
    "    \n",
    "    # Составляем словарь слов предложения\n",
    "    words_dict = defaultdict(int)\n",
    "    for word in words_list:\n",
    "        if word in words_dict:\n",
    "            words_dict[word] += 1\n",
    "        else:\n",
    "            words_dict[word] = 1\n",
    "    \n",
    "    # Находим расстояние только близких слов и только из контекста\n",
    "    cos_result = cos_func(words_dict, ask_word, False)  \n",
    "    if bool(cos_result):\n",
    "        result = max(cos_result.items(), key=operator.itemgetter(1))[0]  # Поиск наилучшего значения\n",
    "        \n",
    "    # Если близкие слова из контекста не найдены, \n",
    "    # то ищем близкие слова с учетом самого слова (понижается точность результата).\n",
    "    else:\n",
    "        words_dict[ask_word] = 1\n",
    "        cos_result = cos_func(words_dict, ask_word, False)\n",
    "        if bool(cos_result):\n",
    "            result = max(cos_result.items(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "        # Если не смогли найти набор близких слов, то ищем через вспомогательные слова\n",
    "        else:\n",
    "            cos_result = cos_func(words_dict, ask_word, True)\n",
    "            result = max(cos_result.items(), key=operator.itemgetter(1))[0]\n",
    "    \n",
    "    #print(cos_result, '\\n')  # Для проверки\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Формируем список синсетов\n",
    "def text_of_synsets(initial_sentences):\n",
    "    \"\"\"Возвращает список предложений, каждое состоит из списка синсетов в соответствии со словом\"\"\"\n",
    "    text_result = list()\n",
    "    for sentence in initial_sentences:\n",
    "        sentence_result = list()\n",
    "        for word in sentence:\n",
    "            if (word in lexicon):\n",
    "                result = cos_similar(sentence, word)\n",
    "                sentence_result.append(result)\n",
    "            else:\n",
    "                sentence_result.append(None)\n",
    "        text_result.append(sentence_result)\n",
    "    return text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Вывод\n",
    "def word_synset_pair(text_result, mystem_sentences):\n",
    "    sentence_index = 0\n",
    "    synset_text = list()\n",
    "    for sentence_result in text_result:\n",
    "        synset_sentence = list()\n",
    "        word_index = 0\n",
    "        for synset_number in sentence_result:\n",
    "            initial_word = mystem_sentences[sentence_index][word_index][0]\n",
    "            if (synset_number is not None):\n",
    "                synset_word = (initial_word, synset_number)\n",
    "            else:\n",
    "                synset_word = (initial_word, None)\n",
    "                \n",
    "            synset_sentence.append(synset_word)\n",
    "            word_index = word_index + 1\n",
    "        \n",
    "        synset_text.append(synset_sentence)\n",
    "        sentence_index = sentence_index + 1\n",
    "    return synset_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'\", None)\n",
      "('Если', 4976)\n",
      "('подросток', 685)\n",
      "('добрый', 2)\n",
      "(',', None)\n",
      "('уступчивый', 2)\n",
      "('и', 1715)\n",
      "('хороший', 583)\n",
      "(',', None)\n",
      "('то', 3591)\n",
      "('это', 3591)\n",
      "('часто', 5195)\n",
      "('воспринимается', 19153)\n",
      "('окружающими', None)\n",
      "('как', 25277)\n",
      "('проявление', 9572)\n",
      "('слабости', 24445)\n",
      "('его', None)\n",
      "('характера', 11944)\n",
      "(',', None)\n",
      "('как', 25277)\n",
      "('неспособность', 24799)\n",
      "('чётко', None)\n",
      "('высказать', 20468)\n",
      "('свою', 12978)\n",
      "('позицию', 28803)\n",
      "('.', None)\n",
      "\n",
      "('Как', 25277)\n",
      "('поживаешь', None)\n",
      "(',', None)\n",
      "('друг', 7888)\n",
      "('?', None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    'Если подросток добрый, уступчивый и хороший, '\n",
    "    + 'то это часто воспринимается окружающими как проявление слабости его характера, '\n",
    "    + 'как неспособность чётко высказать свою позицию. '\n",
    "    + 'Как поживаешь, друг?')\n",
    "\n",
    "mystem_sentences = mystem_func(text)  # Cписок предложений, состоящий из списков [token, lemma, pos] для слов\n",
    "initial_sentences = initial_form(mystem_sentences)  # Cписок предложений со списками слов в начальной форме\n",
    "text_result = text_of_synsets(initial_sentences)  # Cписок предложений со списками синсетов слов\n",
    "result = word_synset_pair(text_result, mystem_sentences) # Список предложений с парой слово-синсет\n",
    "\n",
    "for sentence in result:\n",
    "    for word in sentence:\n",
    "        print(word)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
