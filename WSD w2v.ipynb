{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import platform\n",
    "import csv\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import gensim\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для форматирования вывода\n",
    "BOLD = '\\033[1m'\n",
    "END = '\\033[0m'\n",
    "UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_fpath = \"all.norm-sz100-w10-cb0-it1-min100.w2v\"\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(w2v_fpath, binary=True, unicode_errors='ignore')\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Извлечение слова из строки вида 'word#X:Y'\n",
    "def lexeme(str):\n",
    "    \"\"\"Возвращает word, если вид word#X\n",
    "    Возвращает word, freq, если вид word#X:freq\n",
    "    \"\"\"\n",
    "    if '#' in str:\n",
    "        word, tail = str.split('#', 1)\n",
    "    else:\n",
    "        word, tail = str, None\n",
    "\n",
    "    if tail:\n",
    "        if ':' in tail:\n",
    "            labels, tail = tail.split(':', 1)\n",
    "        else:\n",
    "            labels, tail = tail, None\n",
    "\n",
    "    if tail:\n",
    "        freq = float(tail)\n",
    "    else:\n",
    "        freq = 1\n",
    "    \n",
    "    return word, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synsets = {}  # Словарь {id -> список синсетов}\n",
    "index = defaultdict(list)  # Словарь {слово -> номера синсетов с упоминаниями}\n",
    "lexicon = set()  # Набор всех слов в базе\n",
    "\n",
    "# Считываем файл\n",
    "with open('watset-mcl-mcl-joint-exp-linked.tsv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)  \n",
    "\n",
    "    # Перебираем строки и заполняем переменные synsets и relations словами.\n",
    "    # Ключи - номер строки (с единицы).\n",
    "    # Значения - словари вида {слово -> частота}.\n",
    "    for row in reader:\n",
    "        synsets_dict = dict()\n",
    "        for word in row[2].split(', '):\n",
    "            if word:\n",
    "                key, value = lexeme(word)\n",
    "                synsets_dict[key] = value\n",
    "                \n",
    "        for word in row[4].split(', '):\n",
    "            if word:\n",
    "                key, value = lexeme(word)\n",
    "                synsets_dict[key] = value\n",
    "        synsets[int(row[0])] = synsets_dict\n",
    "        \n",
    "        # Закидываем номер строки в index для каждого слова.\n",
    "        for word in synsets[int(row[0])]:\n",
    "            index[word].append(int(row[0]))\n",
    "\n",
    "        # Обновляем лексикон базы данных\n",
    "        lexicon.update(synsets[int(row[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mystem_func(text):\n",
    "    \"\"\"Возвращает список, состоящий из списков вида [token, lemma, pos]\"\"\"\n",
    "    \n",
    "    coding = 'UTF-8'\n",
    "    if (platform.system() == 'Windows'):\n",
    "        coding = 'cp866'\n",
    "    \n",
    "    # Выполнение команды mystem\n",
    "    #command = \"echo '%s' | ./mystem -e %s -nidsc\" % (text, coding)\n",
    "    command = \"echo '%s' | mystem -e %s -nidsc\" % (text, coding)\n",
    "    proc = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, )\n",
    "    output = proc.communicate()[0] \n",
    "    \n",
    "    # Обработка результата в считываемый вид (массив строк)\n",
    "    string_output = str(output.decode(coding))\n",
    "    \n",
    "    #string_output = string_output[3:]  # В начале вывода стоит кавычка и перевод строки - не значащая информация\n",
    "    string_output = string_output[:-2]  # В конце вывода стоит кавычка и перевод строки - не значащая информация\n",
    "    \n",
    "    string_output = string_output.replace(\"_\", \"\")  # Убираем незначащий символ \"_\" в выводе \n",
    "    sentences = string_output.split('\\s')  # Деление вывода на предложения\n",
    "    \n",
    "    # Обработка каждого предложения по очереди\n",
    "    # Результат каждого предложения - элемент списка sentences_array\n",
    "    sentences_array = list()\n",
    "    for sentence in sentences:\n",
    "        if (platform.system() == 'Windows'):\n",
    "            words = sentence.split(os.linesep)  # Деление предложений на слова\n",
    "        else:\n",
    "            words = sentence.split('\\n')  # Деление предложений на слова\n",
    "        words = [x for x in words if x != '']  # В некоторых местах появляются пустые элементы списка - удаляем\n",
    "        \n",
    "    \n",
    "        # Обработка каждой строки так, чтобы получить для слова его токен, лемму и часть речи.\n",
    "        # Результат для каждого слова записывается в список из трех элементов.\n",
    "        # Все списки хранятся в списке words_array\n",
    "        words_array = list()\n",
    "        for word_line in words:\n",
    "            if (len(word_line) == 1):  #  Случай, если попался знак пунктуации\n",
    "                token = word_line\n",
    "                buf = [word_line, 'PUNC']\n",
    "            \n",
    "            else:  #  Случай, если попалось слово\n",
    "                start_index = word_line.find('{',)\n",
    "                end_index = len(word_line) - 1\n",
    "                token = word_line[:start_index]  \n",
    "                \n",
    "                # Отбрасываем лишнюю информацию\n",
    "                buf = word_line[start_index+1:end_index].split(',')\n",
    "                buf = buf[0].split('=')\n",
    "                if (len(buf) > 2):\n",
    "                    del buf[2]\n",
    "              \n",
    "            # Оформляем результат в виде списка\n",
    "            buf.insert(0, token)\n",
    "            words_array.append(buf)\n",
    "        \n",
    "        sentences_array.append(words_array)\n",
    "    return sentences_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Трансформируем слова в начальную форму для дальнейшего поиска по базе данных\n",
    "def initial_form(sentences):\n",
    "    \"\"\"Возвращает список предложений со списками слов в начальной форме\"\"\"\n",
    "    initial_text_list = list()\n",
    "    for sentence in sentences:\n",
    "        initial_sentence_list = list()\n",
    "        for word_array in sentence:\n",
    "            initial_sentence_list.append(word_array[1])\n",
    "        initial_text_list.append(initial_sentence_list)\n",
    "    return initial_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет плотных векторов для всех синсетов\n",
    "synset_vector_dict = dict()\n",
    "for synset_id in synsets.keys():\n",
    "    vector = numpy.zeros(w2v.vector_size)\n",
    "    vector = vector.reshape(1, -1)\n",
    "    word_count = 0\n",
    "    for word in synsets[synset_id].keys():\n",
    "        try:\n",
    "            vector = vector + synsets[synset_id][word] * w2v[word].reshape(1, -1)\n",
    "            word_count = word_count + 1\n",
    "        except:\n",
    "            continue\n",
    "    if word_count > 0:\n",
    "        synset_vector_dict[synset_id] = vector / word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punc = ['!','?', ',', '.', ';', '\"', \"'\"]\n",
    "\n",
    "# Поиск наилучшего синсета для каждого слова из предложения words_list с помощью векторов\n",
    "def cos_similar(words_list):\n",
    "    \"\"\"Возвращает номер синсета для слова в предложении\"\"\"\n",
    "    \n",
    "    #words_list = [x for x in words_list if x not in punc] # Удаляем знаки пунктуации\n",
    "    \n",
    "    sentence_vector = numpy.zeros(w2v.vector_size)\n",
    "    sentence_vector = sentence_vector.reshape(1, -1)\n",
    "    word_count = 0\n",
    "    sentence_result = list()\n",
    "    \n",
    "    # Расчет плотного вектора для предложения\n",
    "    for word in words_list:\n",
    "        try:\n",
    "            sentence_vector = sentence_vector + w2v[word].reshape(1, -1)\n",
    "            word_count = word_count + 1\n",
    "        except:\n",
    "            continue\n",
    "    if word_count > 0:\n",
    "        sentence_vector = sentence_vector / word_count\n",
    "    \n",
    "    # Поиск наилучшего синсета для каждого слова из предложения\n",
    "    for word in words_list:\n",
    "        if word not in punc:\n",
    "            sim_max_result = 0\n",
    "            answer = 0\n",
    "            for synset_number in index[word]:\n",
    "                synset = synsets[synset_number]\n",
    "                sim = cosine_similarity(synset_vector_dict[synset_number], sentence_vector).item(0)\n",
    "                if sim > sim_max_result:\n",
    "                    sim_max_result = sim\n",
    "                    answer = synset_number\n",
    "            if answer != 0:\n",
    "                sentence_result.append(answer)\n",
    "            else:\n",
    "                sentence_result.append(None)\n",
    "        else:\n",
    "            sentence_result.append(None)\n",
    "            \n",
    "    return sentence_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Формируем список синсетов\n",
    "def text_of_synsets(initial_sentences):\n",
    "    \"\"\"Возвращает список предложений, каждое состоит из списка синсетов в соответствии со словом\"\"\"\n",
    "    text_result = list()\n",
    "    for sentence in initial_sentences:\n",
    "        sentence_result = cos_similar(sentence)\n",
    "        text_result.append(sentence_result)\n",
    "    return text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Вывод\n",
    "def word_synset_pair(text_result, mystem_sentences):\n",
    "    sentence_index = 0\n",
    "    synset_text = list()\n",
    "    for sentence_result in text_result:\n",
    "        synset_sentence = list()\n",
    "        word_index = 0\n",
    "        for synset_number in sentence_result:\n",
    "            initial_word = mystem_sentences[sentence_index][word_index][0]\n",
    "            if (synset_number is not None):\n",
    "                synset_word = (initial_word, synset_number)\n",
    "            else:\n",
    "                synset_word = (initial_word, None)\n",
    "                \n",
    "            synset_sentence.append(synset_word)\n",
    "            word_index = word_index + 1\n",
    "        \n",
    "        synset_text.append(synset_sentence)\n",
    "        sentence_index = sentence_index + 1\n",
    "    return synset_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'\", None)\n",
      "('Если', 4976)\n",
      "('подросток', 77)\n",
      "('добрый', 583)\n",
      "(',', None)\n",
      "('уступчивый', 2)\n",
      "('и', 810)\n",
      "('хороший', 137)\n",
      "(',', None)\n",
      "('то', 35127)\n",
      "('это', 3591)\n",
      "('часто', 5195)\n",
      "('воспринимается', 540)\n",
      "('окружающими', None)\n",
      "('как', 25276)\n",
      "('проявление', 20433)\n",
      "('слабости', 34341)\n",
      "('его', None)\n",
      "('характера', 3591)\n",
      "(',', None)\n",
      "('как', 25276)\n",
      "('неспособность', 24799)\n",
      "('чётко', None)\n",
      "('высказать', 305)\n",
      "('свою', 12978)\n",
      "('позицию', 33156)\n",
      "('.', None)\n",
      "\n",
      "('Как', 2607)\n",
      "('поживаешь', None)\n",
      "(',', None)\n",
      "('друг', 312)\n",
      "('?', None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    'Если подросток добрый, уступчивый и хороший, '\n",
    "    + 'то это часто воспринимается окружающими как проявление слабости его характера, '\n",
    "    + 'как неспособность чётко высказать свою позицию. '\n",
    "    + 'Как поживаешь, друг?')\n",
    "\n",
    "mystem_sentences = mystem_func(text)  # Cписок предложений, состоящий из списков [token, lemma, pos] для слов\n",
    "initial_sentences = initial_form(mystem_sentences)  # Cписок предложений со списками слов в начальной форме\n",
    "text_result = text_of_synsets(initial_sentences)  # Cписок предложений со списками синсетов слов\n",
    "result = word_synset_pair(text_result, mystem_sentences) # Список предложений с парой слово-синсет\n",
    "\n",
    "for sentence in result:\n",
    "    for word in sentence:\n",
    "        print(word)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
